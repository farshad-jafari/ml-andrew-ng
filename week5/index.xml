<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته پنجم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/</link>
    <description>Recent content in  هفته پنجم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Sep 2020 17:44:01 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>تابع هزینه شبکه عصبی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</link>
      <pubDate>Wed, 30 Sep 2020 17:52:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</guid>
      <description>اجازه دهید برای شروع چند متغیر تعریف کنیم:
   متغیر      $L$ تعداد کل لایه ها در شبکه عصبی   $s_l$ تعداد نود ها در لایه $l$ ام (بدون احتساب نود بایاس)   $K$ تعداد کلاس های خروجی    به یاد بیاورید که در شبکه های عصبی ممکن است تعداد نود های خروجی زیادی داشته باشیم. ما $h_\Theta(x)_k$ را به عنوان یک فرضیه در نظر می‌گیریم، که منجر به خروجی $k^{th}$ می‌شود.</description>
    </item>
    
    <item>
      <title>Backpropagation</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation/</link>
      <pubDate>Thu, 01 Oct 2020 12:30:31 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation/</guid>
      <description>اصطلاح Backpropagation یا به فارسی پس انتشار در شبکه های عصبی، برای به حداقل رساندن تابع هزینه (minimizing cost function) مثل کاری که با گرادیان کاهشی در رگرسیون لجستیک و خطی انجام می‌دادیم، استفاده می‌شود.
هدف ما محاسبه این است:
$$ min_\Theta J(\Theta) $$
یعنی می‌خواهیم تابع هزیه $J$ را با استفاده از یک محموعه بهینه از پارامتر $\Theta$ به حداقل برسانیم (یا به عبارتی دیگر مینیمم کنیم).
در این بخش به معادلاتی که برای محاسبه مشتق جزئی تابع $J(\Theta)$ استفاده می‌کنیم، خواهیم پرداخت:</description>
    </item>
    
  </channel>
</rss>