<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته پنجم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/</link>
    <description>Recent content in  هفته پنجم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Sep 2020 17:44:01 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>تابع هزینه شبکه عصبی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</link>
      <pubDate>Wed, 30 Sep 2020 17:52:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</guid>
      <description>اجازه دهید برای شروع چند متغیر تعریف کنیم:
   متغیر      $L$ تعداد کل لایه ها در شبکه عصبی   $s_l$ تعداد نود ها در لایه $l$ ام (بدون احتساب نود بایاس)   $K$ تعداد کلاس های خروجی    به یاد بیاورید که در شبکه های عصبی ممکن است تعداد نود های خروجی زیادی داشته باشیم. ما $h_\Theta(x)_k$ را به عنوان یک فرضیه در نظر می‌گیریم، که منجر به خروجی $k^{th}$ می‌شود.</description>
    </item>
    
    <item>
      <title>Backpropagation</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation/</link>
      <pubDate>Thu, 01 Oct 2020 12:30:31 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>