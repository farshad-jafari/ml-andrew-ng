<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> هفته پنجم on Machine Learning Andrew Ng</title>
    <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/</link>
    <description>Recent content in  هفته پنجم on Machine Learning Andrew Ng</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Sep 2020 17:44:01 +0330</lastBuildDate>
    
	<atom:link href="https://mehrdad-dev.github.io/ml-andrew-ng/week5/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>تابع هزینه شبکه عصبی</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</link>
      <pubDate>Wed, 30 Sep 2020 17:52:45 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/nn-cost-function/</guid>
      <description>اجازه دهید برای شروع چند متغیر تعریف کنیم:
   متغیر      $L$ تعداد کل لایه ها در شبکه عصبی   $s_l$ تعداد نود ها در لایه $l$ ام (بدون احتساب نود بایاس)   $K$ تعداد کلاس های خروجی    به یاد بیاورید که در شبکه های عصبی ممکن است تعداد نود های خروجی زیادی داشته باشیم. ما $h_\Theta(x)_k$ را به عنوان یک فرضیه در نظر می‌گیریم، که منجر به خروجی $k^{th}$ می‌شود.</description>
    </item>
    
    <item>
      <title>Backpropagation قسمت اول</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</link>
      <pubDate>Thu, 01 Oct 2020 12:30:31 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-1/</guid>
      <description>اصطلاح Backpropagation یا به فارسی پس انتشار در شبکه های عصبی، برای به حداقل رساندن تابع هزینه (minimizing cost function) مثل کاری که با گرادیان کاهشی در رگرسیون لجستیک و خطی انجام می‌دادیم، استفاده می‌شود.
هدف ما محاسبه این است:
$$ min_\Theta J(\Theta) $$
یعنی می‌خواهیم تابع هزیه $J$ را با استفاده از یک محموعه بهینه از پارامتر $\Theta$ به حداقل برسانیم (یا به عبارتی دیگر مینیمم کنیم).
در این بخش به معادلاتی که برای محاسبه مشتق جزئی تابع $J(\Theta)$ استفاده می‌کنیم، خواهیم پرداخت:</description>
    </item>
    
    <item>
      <title>Backpropagation قسمت دوم</title>
      <link>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</link>
      <pubDate>Thu, 08 Oct 2020 12:25:51 +0330</pubDate>
      
      <guid>https://mehrdad-dev.github.io/ml-andrew-ng/week5/backpropagation-2/</guid>
      <description>به یاد بیاورید که تابع هزینه برای یک شبکه عصبی به این صورت بود:
$$ \begin{gather*}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather*} $$</description>
    </item>
    
  </channel>
</rss>